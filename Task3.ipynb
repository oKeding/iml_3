{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and get data as letters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "x_train = pd.read_csv('train.csv')\n",
    "\n",
    "#x_train = x_train[0:10000]\n",
    "\n",
    "letters = x_train['Sequence'].apply(lambda x: pd.Series(list(x)))\n",
    "x_train = x_train.drop('Sequence', axis = 1)\n",
    "x_train = x_train.join(letters)\n",
    "print(x_train)\n",
    "\n",
    "x_test = pd.read_csv('test.csv')\n",
    "letters = x_test['Sequence'].apply(lambda x: pd.Series(list(x)))\n",
    "x_test = x_test.drop('Sequence', axis = 1)\n",
    "x_test = x_test.join(letters)\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_oh = pd.get_dummies(data=x_train, columns=[0, 1, 2, 3], sparse=True)\n",
    "print(x_train_oh)\n",
    "\n",
    "x_test_oh = pd.get_dummies(data=x_test, columns=[0, 1, 2, 3], sparse=True)\n",
    "print(x_test_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112000"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: fix unbalanced data for better performance. Easiest is to oversample the 1's. Its worth a shot at least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Neural network <-- there is documentation in scikit\n",
    "l1_size = int(0.006 * x_train_oh.shape[0])\n",
    "l2_size = int(0.008 * x_train_oh.shape[0])\n",
    "clf = MLPClassifier(solver='adam', alpha=1e-3, hidden_layer_sizes=(l1_size, l2_size), random_state=1, activation='tanh', max_iter=200)\n",
    "#clf.fit(x_train_oh.iloc[:, 1:], x_train_oh['Active'])\n",
    "#pred = clf.predict(x_test_oh)\n",
    "#print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe split into training and validating sets?\n",
    "#df = pd.DataFrame(pred[1:], columns=[1]) \n",
    "#print(df)\n",
    "#df.to_csv('out.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV F1 score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "f1_scores = []\n",
    "for train_indices, test_indices in kf.split(x_train):\n",
    "    clf.fit(x_train_oh.iloc[train_indices, 1:], x_train_oh.iloc[train_indices, 0])\n",
    "    pred = clf.predict(x_train_oh.iloc[test_indices, 1:])\n",
    "    #print(clf.score(x_train_oh.iloc[test_indices, 1:], x_train_oh.iloc[test_indices, 0]))\n",
    "    f1_scores.append(f1_score(pred, x_train_oh.iloc[test_indices, 0]))\n",
    "    print(f1_score(pred, x_train_oh.iloc[test_indices, 0]))\n",
    "print(\"Mean F1 score:\", np.array(f1_scores).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot convergence of solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search to find parameters that give best F1 score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters {'hidden_layer_sizes': (60, 80)} gives best score: 0.7540308323707317\n",
      "{'mean_fit_time': array([19.00803643, 19.31100678, 15.98581493, 17.03055614, 22.3140865 ]), 'std_fit_time': array([0.28902677, 0.14915076, 0.19945387, 0.10223015, 0.12321142]), 'mean_score_time': array([0.15239179, 0.03189653, 0.0651148 , 0.03495723, 0.04025084]), 'std_score_time': array([0.13857244, 0.00214542, 0.01033127, 0.00980135, 0.02924387]), 'param_hidden_layer_sizes': masked_array(data=[(60, 80), (50, 80), (70, 80), (60, 70), (60, 90)],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'hidden_layer_sizes': (60, 80)}, {'hidden_layer_sizes': (50, 80)}, {'hidden_layer_sizes': (70, 80)}, {'hidden_layer_sizes': (60, 70)}, {'hidden_layer_sizes': (60, 90)}], 'split0_test_score': array([0.74725275, 0.63783784, 0.68478261, 0.73142857, 0.73195876]), 'split1_test_score': array([0.78651685, 0.74576271, 0.74576271, 0.75449102, 0.77837838]), 'split2_test_score': array([0.77844311, 0.71165644, 0.75739645, 0.78527607, 0.79041916]), 'split3_test_score': array([0.70391061, 0.70157068, 0.69791667, 0.68508287, 0.68085106]), 'mean_test_score': array([0.75403083, 0.69920692, 0.72146461, 0.73906963, 0.74540184]), 'std_test_score': array([0.03243948, 0.03903263, 0.03074723, 0.03655752, 0.04319008]), 'rank_test_score': array([1, 5, 4, 3, 2], dtype=int32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martinbrandt/opt/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(random_state=1, alpha=1e-3, max_iter=100, solver='adam', activation='tanh')\n",
    "\n",
    "# All parameters we want to try:\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(60, 80), (50, 80), (70, 80), (60, 70), (60, 90)],\n",
    "    #'activation': ['tanh', 'relu', 'logistic'],\n",
    "    #'solver': ['sgd', 'adam'],\n",
    "    #'alpha': [1e-3],\n",
    "}\n",
    "\n",
    "# 10000 * 0.006\n",
    "\n",
    "# Do grid search over all parameter options:\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=4, scoring='f1')\n",
    "clf.fit(x_train_oh.iloc[:, 1:], x_train_oh['Active'])\n",
    "print('Parameters', clf.best_params_, \"gives best score:\", clf.best_score_)\n",
    "print(clf.cv_results_.mean_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \"the optimal size of the hidden layer is usually between the size of the input and size of the output layers\"\n",
    "#  \"number of neurons = 0.005 * number of samples?\" 2/3 size of input is also usual\n",
    "\n",
    "# Easy: 0.607427055703\n",
    "# Medium: 0.852643419573\n",
    "# Hard: 0.89591280654"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
