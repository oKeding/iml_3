{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and get data as letters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Active  0  1  2  3\n",
      "0            0  D  K  W  L\n",
      "1            0  F  C  H  N\n",
      "2            0  K  D  Q  P\n",
      "3            0  F  N  W  I\n",
      "4            0  N  K  R  M\n",
      "...        ... .. .. .. ..\n",
      "111995       0  G  S  M  E\n",
      "111996       0  D  L  P  T\n",
      "111997       0  S  G  H  C\n",
      "111998       0  K  I  G  T\n",
      "111999       0  P  G  P  T\n",
      "\n",
      "[112000 rows x 5 columns]\n",
      "       0  1  2  3\n",
      "0      H  W  F  K\n",
      "1      M  W  P  W\n",
      "2      A  L  D  V\n",
      "3      N  T  L  G\n",
      "4      L  H  Y  Y\n",
      "...   .. .. .. ..\n",
      "47995  N  R  W  M\n",
      "47996  M  M  M  K\n",
      "47997  A  F  N  M\n",
      "47998  C  R  Y  I\n",
      "47999  M  K  F  C\n",
      "\n",
      "[48000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "test = True\n",
    "\n",
    "x_train = pd.read_csv('train.csv')\n",
    "\n",
    "#x_train = x_train[0:60000]\n",
    "\n",
    "letters = x_train['Sequence'].apply(lambda x: pd.Series(list(x)))\n",
    "x_train = x_train.drop('Sequence', axis = 1)\n",
    "x_train = x_train.join(letters)\n",
    "print(x_train)\n",
    "\n",
    "if test:\n",
    "    x_test = pd.read_csv('test.csv')\n",
    "    letters = x_test['Sequence'].apply(lambda x: pd.Series(list(x)))\n",
    "    x_test = x_test.drop('Sequence', axis = 1)\n",
    "    x_test = x_test.join(letters)\n",
    "    print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Active  0_A  0_C  0_D  0_E  0_F  0_G  0_H  0_I  0_K  ...  3_M  3_N  \\\n",
      "0            0    0    0    1    0    0    0    0    0    0  ...    0    0   \n",
      "1            0    0    0    0    0    1    0    0    0    0  ...    0    1   \n",
      "2            0    0    0    0    0    0    0    0    0    1  ...    0    0   \n",
      "3            0    0    0    0    0    1    0    0    0    0  ...    0    0   \n",
      "4            0    0    0    0    0    0    0    0    0    0  ...    1    0   \n",
      "...        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "111995       0    0    0    0    0    0    1    0    0    0  ...    0    0   \n",
      "111996       0    0    0    1    0    0    0    0    0    0  ...    0    0   \n",
      "111997       0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
      "111998       0    0    0    0    0    0    0    0    0    1  ...    0    0   \n",
      "111999       0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
      "\n",
      "        3_P  3_Q  3_R  3_S  3_T  3_V  3_W  3_Y  \n",
      "0         0    0    0    0    0    0    0    0  \n",
      "1         0    0    0    0    0    0    0    0  \n",
      "2         1    0    0    0    0    0    0    0  \n",
      "3         0    0    0    0    0    0    0    0  \n",
      "4         0    0    0    0    0    0    0    0  \n",
      "...     ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "111995    0    0    0    0    0    0    0    0  \n",
      "111996    0    0    0    0    1    0    0    0  \n",
      "111997    0    0    0    0    0    0    0    0  \n",
      "111998    0    0    0    0    1    0    0    0  \n",
      "111999    0    0    0    0    1    0    0    0  \n",
      "\n",
      "[112000 rows x 81 columns]\n",
      "       0_A  0_C  0_D  0_E  0_F  0_G  0_H  0_I  0_K  0_L  ...  3_M  3_N  3_P  \\\n",
      "0        0    0    0    0    0    0    1    0    0    0  ...    0    0    0   \n",
      "1        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "2        1    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "3        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "4        0    0    0    0    0    0    0    0    0    1  ...    0    0    0   \n",
      "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "47995    0    0    0    0    0    0    0    0    0    0  ...    1    0    0   \n",
      "47996    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "47997    1    0    0    0    0    0    0    0    0    0  ...    1    0    0   \n",
      "47998    0    1    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "47999    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "\n",
      "       3_Q  3_R  3_S  3_T  3_V  3_W  3_Y  \n",
      "0        0    0    0    0    0    0    0  \n",
      "1        0    0    0    0    0    1    0  \n",
      "2        0    0    0    0    1    0    0  \n",
      "3        0    0    0    0    0    0    0  \n",
      "4        0    0    0    0    0    0    1  \n",
      "...    ...  ...  ...  ...  ...  ...  ...  \n",
      "47995    0    0    0    0    0    0    0  \n",
      "47996    0    0    0    0    0    0    0  \n",
      "47997    0    0    0    0    0    0    0  \n",
      "47998    0    0    0    0    0    0    0  \n",
      "47999    0    0    0    0    0    0    0  \n",
      "\n",
      "[48000 rows x 80 columns]\n"
     ]
    }
   ],
   "source": [
    "x_train_oh = pd.get_dummies(data=x_train, columns=[0, 1, 2, 3], sparse=True)\n",
    "print(x_train_oh)\n",
    "\n",
    "if test:\n",
    "    x_test_oh = pd.get_dummies(data=x_test, columns=[0, 1, 2, 3], sparse=True)\n",
    "    print(x_test_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix unbalanced data for better performance. Easiest is to oversample the 1's. Its worth a shot at least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Neural network <-- there is documentation in scikit\n",
    "l1_size = int(0.003 * x_train_oh.shape[0])\n",
    "l2_size = int(0.002 * x_train_oh.shape[0])\n",
    "clf = MLPClassifier(solver='adam', alpha=8e-4, verbose=10, hidden_layer_sizes=(l1_size,l2_size), random_state=1, activation='tanh', max_iter=400, tol=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV F1 score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.09410011\n",
      "Iteration 2, loss = 0.06921685\n",
      "Iteration 3, loss = 0.06852413\n",
      "Iteration 4, loss = 0.06713904\n",
      "Iteration 5, loss = 0.06482222\n",
      "Iteration 6, loss = 0.05972665\n",
      "Iteration 7, loss = 0.05309142\n",
      "Iteration 8, loss = 0.04733377\n",
      "Iteration 9, loss = 0.04381315\n",
      "Iteration 10, loss = 0.04171600\n",
      "Iteration 11, loss = 0.04034404\n",
      "Iteration 12, loss = 0.03908368\n",
      "Iteration 13, loss = 0.03804557\n",
      "Iteration 14, loss = 0.03647994\n",
      "Iteration 15, loss = 0.03511197\n",
      "Iteration 16, loss = 0.03407346\n",
      "Iteration 17, loss = 0.03227091\n",
      "Iteration 18, loss = 0.03136344\n",
      "Iteration 19, loss = 0.02979605\n",
      "Iteration 20, loss = 0.02928689\n",
      "Iteration 21, loss = 0.02808438\n",
      "Iteration 22, loss = 0.02701508\n",
      "Iteration 23, loss = 0.02606442\n",
      "Iteration 24, loss = 0.02488144\n",
      "Iteration 25, loss = 0.02378730\n",
      "Iteration 26, loss = 0.02311449\n",
      "Iteration 27, loss = 0.02252202\n",
      "Iteration 28, loss = 0.02146177\n",
      "Iteration 29, loss = 0.02036859\n",
      "Iteration 30, loss = 0.01992482\n",
      "Iteration 31, loss = 0.01902458\n",
      "Iteration 32, loss = 0.01802458\n",
      "Iteration 33, loss = 0.01731077\n",
      "Iteration 34, loss = 0.01646403\n",
      "Iteration 35, loss = 0.01560323\n",
      "Iteration 36, loss = 0.01493568\n",
      "Iteration 37, loss = 0.01443928\n",
      "Iteration 38, loss = 0.01324090\n",
      "Iteration 39, loss = 0.01280257\n",
      "Iteration 40, loss = 0.01211569\n",
      "Iteration 41, loss = 0.01130405\n",
      "Iteration 42, loss = 0.01071294\n",
      "Iteration 43, loss = 0.01030470\n",
      "Iteration 44, loss = 0.00940780\n",
      "Iteration 45, loss = 0.00953349\n",
      "Iteration 46, loss = 0.00845929\n",
      "Iteration 47, loss = 0.00814091\n",
      "Iteration 48, loss = 0.00749766\n",
      "Iteration 49, loss = 0.00705706\n",
      "Iteration 50, loss = 0.00680121\n",
      "Iteration 51, loss = 0.00652355\n",
      "Iteration 52, loss = 0.00640505\n",
      "Iteration 53, loss = 0.00608017\n",
      "Iteration 54, loss = 0.00557404\n",
      "Iteration 55, loss = 0.00555867\n",
      "Iteration 56, loss = 0.00541419\n",
      "Iteration 57, loss = 0.00528246\n",
      "Iteration 58, loss = 0.00498460\n",
      "Iteration 59, loss = 0.00487268\n",
      "Iteration 60, loss = 0.00463495\n",
      "Iteration 61, loss = 0.00464166\n",
      "Iteration 62, loss = 0.00482221\n",
      "Iteration 63, loss = 0.00436320\n",
      "Iteration 64, loss = 0.00426966\n",
      "Iteration 65, loss = 0.00427924\n",
      "Iteration 66, loss = 0.00443575\n",
      "Iteration 67, loss = 0.00564315\n",
      "Iteration 68, loss = 0.00701276\n",
      "Iteration 69, loss = 0.00436509\n",
      "Iteration 70, loss = 0.00380573\n",
      "Iteration 71, loss = 0.00369593\n",
      "Iteration 72, loss = 0.00361815\n",
      "Iteration 73, loss = 0.00356139\n",
      "Iteration 74, loss = 0.00352892\n",
      "Iteration 75, loss = 0.00348151\n",
      "Iteration 76, loss = 0.00345024\n",
      "Iteration 77, loss = 0.00354394\n",
      "Iteration 78, loss = 0.00837325\n",
      "Iteration 79, loss = 0.00570409\n",
      "Iteration 80, loss = 0.00398565\n",
      "Iteration 81, loss = 0.00355593\n",
      "Iteration 82, loss = 0.00342528\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8692660550458715\n",
      "Iteration 1, loss = 0.09417419\n",
      "Iteration 2, loss = 0.07063553\n",
      "Iteration 3, loss = 0.06995332\n",
      "Iteration 4, loss = 0.06864437\n",
      "Iteration 5, loss = 0.06606171\n",
      "Iteration 6, loss = 0.06064346\n",
      "Iteration 7, loss = 0.05389515\n",
      "Iteration 8, loss = 0.04814612\n",
      "Iteration 9, loss = 0.04512867\n",
      "Iteration 10, loss = 0.04270001\n",
      "Iteration 11, loss = 0.04057262\n",
      "Iteration 12, loss = 0.03920433\n",
      "Iteration 13, loss = 0.03757135\n",
      "Iteration 14, loss = 0.03547102\n",
      "Iteration 15, loss = 0.03456757\n",
      "Iteration 16, loss = 0.03348519\n",
      "Iteration 17, loss = 0.03159633\n",
      "Iteration 18, loss = 0.03104330\n",
      "Iteration 19, loss = 0.02944419\n",
      "Iteration 20, loss = 0.02836466\n",
      "Iteration 21, loss = 0.02755468\n",
      "Iteration 22, loss = 0.02661571\n",
      "Iteration 23, loss = 0.02536067\n",
      "Iteration 24, loss = 0.02439429\n",
      "Iteration 25, loss = 0.02287346\n",
      "Iteration 26, loss = 0.02283153\n",
      "Iteration 27, loss = 0.02200626\n",
      "Iteration 28, loss = 0.02093222\n",
      "Iteration 29, loss = 0.01992837\n",
      "Iteration 30, loss = 0.01930164\n",
      "Iteration 31, loss = 0.01834725\n",
      "Iteration 32, loss = 0.01716401\n",
      "Iteration 33, loss = 0.01664965\n",
      "Iteration 34, loss = 0.01606947\n",
      "Iteration 35, loss = 0.01575477\n",
      "Iteration 36, loss = 0.01495543\n",
      "Iteration 37, loss = 0.01387052\n",
      "Iteration 38, loss = 0.01309912\n",
      "Iteration 39, loss = 0.01255101\n",
      "Iteration 40, loss = 0.01224501\n",
      "Iteration 41, loss = 0.01122575\n",
      "Iteration 42, loss = 0.01048955\n",
      "Iteration 43, loss = 0.01038717\n",
      "Iteration 44, loss = 0.00952593\n",
      "Iteration 45, loss = 0.00929442\n",
      "Iteration 46, loss = 0.00874664\n",
      "Iteration 47, loss = 0.00873352\n",
      "Iteration 48, loss = 0.00761574\n",
      "Iteration 49, loss = 0.00746175\n",
      "Iteration 50, loss = 0.00712020\n",
      "Iteration 51, loss = 0.00663160\n",
      "Iteration 52, loss = 0.00684383\n",
      "Iteration 53, loss = 0.00620930\n",
      "Iteration 54, loss = 0.00611257\n",
      "Iteration 55, loss = 0.00588183\n",
      "Iteration 56, loss = 0.00570654\n",
      "Iteration 57, loss = 0.00521377\n",
      "Iteration 58, loss = 0.00531189\n",
      "Iteration 59, loss = 0.00520888\n",
      "Iteration 60, loss = 0.00491581\n",
      "Iteration 61, loss = 0.00575668\n",
      "Iteration 62, loss = 0.00523427\n",
      "Iteration 63, loss = 0.00491519\n",
      "Iteration 64, loss = 0.00447796\n",
      "Iteration 65, loss = 0.00418207\n",
      "Iteration 66, loss = 0.00408492\n",
      "Iteration 67, loss = 0.00408053\n",
      "Iteration 68, loss = 0.00412324\n",
      "Iteration 69, loss = 0.00879493\n",
      "Iteration 70, loss = 0.00698843\n",
      "Iteration 71, loss = 0.00430495\n",
      "Iteration 72, loss = 0.00391782\n",
      "Iteration 73, loss = 0.00381239\n",
      "Iteration 74, loss = 0.00370788\n",
      "Iteration 75, loss = 0.00364871\n",
      "Iteration 76, loss = 0.00362514\n",
      "Iteration 77, loss = 0.00368213\n",
      "Iteration 78, loss = 0.00395455\n",
      "Iteration 79, loss = 0.00424211\n",
      "Iteration 80, loss = 0.00982344\n",
      "Iteration 81, loss = 0.00499761\n",
      "Iteration 82, loss = 0.00380340\n",
      "Iteration 83, loss = 0.00360573\n",
      "Iteration 84, loss = 0.00352747\n",
      "Iteration 85, loss = 0.00346703\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.889406286379511\n",
      "Iteration 1, loss = 0.09232326\n",
      "Iteration 2, loss = 0.06906640\n",
      "Iteration 3, loss = 0.06787852\n",
      "Iteration 4, loss = 0.06662755\n",
      "Iteration 5, loss = 0.06405439\n",
      "Iteration 6, loss = 0.05948407\n",
      "Iteration 7, loss = 0.05291996\n",
      "Iteration 8, loss = 0.04682791\n",
      "Iteration 9, loss = 0.04406677\n",
      "Iteration 10, loss = 0.04244998\n",
      "Iteration 11, loss = 0.04038548\n",
      "Iteration 12, loss = 0.03867391\n",
      "Iteration 13, loss = 0.03665116\n",
      "Iteration 14, loss = 0.03487382\n",
      "Iteration 15, loss = 0.03325325\n",
      "Iteration 16, loss = 0.03152592\n",
      "Iteration 17, loss = 0.03011441\n",
      "Iteration 18, loss = 0.02889898\n",
      "Iteration 19, loss = 0.02788355\n",
      "Iteration 20, loss = 0.02665466\n",
      "Iteration 21, loss = 0.02582031\n",
      "Iteration 22, loss = 0.02486127\n",
      "Iteration 23, loss = 0.02400826\n",
      "Iteration 24, loss = 0.02344135\n",
      "Iteration 25, loss = 0.02145984\n",
      "Iteration 26, loss = 0.02099633\n",
      "Iteration 27, loss = 0.02036319\n",
      "Iteration 28, loss = 0.01943741\n",
      "Iteration 29, loss = 0.01802049\n",
      "Iteration 30, loss = 0.01759127\n",
      "Iteration 31, loss = 0.01668837\n",
      "Iteration 32, loss = 0.01563277\n",
      "Iteration 33, loss = 0.01473536\n",
      "Iteration 34, loss = 0.01472666\n",
      "Iteration 35, loss = 0.01410070\n",
      "Iteration 36, loss = 0.01340029\n",
      "Iteration 37, loss = 0.01219149\n",
      "Iteration 38, loss = 0.01125900\n",
      "Iteration 39, loss = 0.01084730\n",
      "Iteration 40, loss = 0.01066349\n",
      "Iteration 41, loss = 0.00991256\n",
      "Iteration 42, loss = 0.00895835\n",
      "Iteration 43, loss = 0.00885417\n",
      "Iteration 44, loss = 0.00821700\n",
      "Iteration 45, loss = 0.00794457\n",
      "Iteration 46, loss = 0.00742701\n",
      "Iteration 47, loss = 0.00720847\n",
      "Iteration 48, loss = 0.00675985\n",
      "Iteration 49, loss = 0.00630564\n",
      "Iteration 50, loss = 0.00619246\n",
      "Iteration 51, loss = 0.00564923\n",
      "Iteration 52, loss = 0.00542752\n",
      "Iteration 53, loss = 0.00562793\n",
      "Iteration 54, loss = 0.00540044\n",
      "Iteration 55, loss = 0.00511645\n",
      "Iteration 56, loss = 0.00515961\n",
      "Iteration 57, loss = 0.00483567\n",
      "Iteration 58, loss = 0.00478151\n",
      "Iteration 59, loss = 0.00456168\n",
      "Iteration 60, loss = 0.00541399\n",
      "Iteration 61, loss = 0.00534694\n",
      "Iteration 62, loss = 0.00433109\n",
      "Iteration 63, loss = 0.00392179\n",
      "Iteration 64, loss = 0.00380831\n",
      "Iteration 65, loss = 0.00375436\n",
      "Iteration 66, loss = 0.00377886\n",
      "Iteration 67, loss = 0.00421403\n",
      "Iteration 68, loss = 0.00849510\n",
      "Iteration 69, loss = 0.00623077\n",
      "Iteration 70, loss = 0.00403284\n",
      "Iteration 71, loss = 0.00370958\n",
      "Iteration 72, loss = 0.00358924\n",
      "Iteration 73, loss = 0.00354166\n",
      "Iteration 74, loss = 0.00346885\n",
      "Iteration 75, loss = 0.00342555\n",
      "Iteration 76, loss = 0.00339988\n",
      "Iteration 77, loss = 0.00338070\n",
      "Iteration 78, loss = 0.00341285\n",
      "Iteration 79, loss = 0.00896617\n",
      "Iteration 80, loss = 0.00739254\n",
      "Iteration 81, loss = 0.00405296\n",
      "Iteration 82, loss = 0.00353034\n",
      "Iteration 83, loss = 0.00341164\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8571428571428571\n",
      "Iteration 1, loss = 0.09112473\n",
      "Iteration 2, loss = 0.06896071\n",
      "Iteration 3, loss = 0.06750250\n",
      "Iteration 4, loss = 0.06620253\n",
      "Iteration 5, loss = 0.06340671\n",
      "Iteration 6, loss = 0.05890043\n",
      "Iteration 7, loss = 0.05204957\n",
      "Iteration 8, loss = 0.04639392\n",
      "Iteration 9, loss = 0.04305700\n",
      "Iteration 10, loss = 0.04088272\n",
      "Iteration 11, loss = 0.03922892\n",
      "Iteration 12, loss = 0.03774947\n",
      "Iteration 13, loss = 0.03694098\n",
      "Iteration 14, loss = 0.03536747\n",
      "Iteration 15, loss = 0.03437985\n",
      "Iteration 16, loss = 0.03313023\n",
      "Iteration 17, loss = 0.03159372\n",
      "Iteration 18, loss = 0.03085370\n",
      "Iteration 19, loss = 0.02926045\n",
      "Iteration 20, loss = 0.02804773\n",
      "Iteration 21, loss = 0.02721434\n",
      "Iteration 22, loss = 0.02662399\n",
      "Iteration 23, loss = 0.02473203\n",
      "Iteration 24, loss = 0.02412205\n",
      "Iteration 25, loss = 0.02271848\n",
      "Iteration 26, loss = 0.02204748\n",
      "Iteration 27, loss = 0.02119066\n",
      "Iteration 28, loss = 0.02002431\n",
      "Iteration 29, loss = 0.01921070\n",
      "Iteration 30, loss = 0.01857154\n",
      "Iteration 31, loss = 0.01763925\n",
      "Iteration 32, loss = 0.01657380\n",
      "Iteration 33, loss = 0.01606596\n",
      "Iteration 34, loss = 0.01540936\n",
      "Iteration 35, loss = 0.01522833\n",
      "Iteration 36, loss = 0.01404140\n",
      "Iteration 37, loss = 0.01357273\n",
      "Iteration 38, loss = 0.01244766\n",
      "Iteration 39, loss = 0.01194555\n",
      "Iteration 40, loss = 0.01162190\n",
      "Iteration 41, loss = 0.01069813\n",
      "Iteration 42, loss = 0.01018560\n",
      "Iteration 43, loss = 0.00981022\n",
      "Iteration 44, loss = 0.00903035\n",
      "Iteration 45, loss = 0.00889277\n",
      "Iteration 46, loss = 0.00818182\n",
      "Iteration 47, loss = 0.00828407\n",
      "Iteration 48, loss = 0.00754620\n",
      "Iteration 49, loss = 0.00720568\n",
      "Iteration 50, loss = 0.00672442\n",
      "Iteration 51, loss = 0.00634157\n",
      "Iteration 52, loss = 0.00599493\n",
      "Iteration 53, loss = 0.00612251\n",
      "Iteration 54, loss = 0.00604016\n",
      "Iteration 55, loss = 0.00561066\n",
      "Iteration 56, loss = 0.00579516\n",
      "Iteration 57, loss = 0.00522299\n",
      "Iteration 58, loss = 0.00483480\n",
      "Iteration 59, loss = 0.00470514\n",
      "Iteration 60, loss = 0.00492069\n",
      "Iteration 61, loss = 0.00458821\n",
      "Iteration 62, loss = 0.00443649\n",
      "Iteration 63, loss = 0.00422833\n",
      "Iteration 64, loss = 0.00418434\n",
      "Iteration 65, loss = 0.00402325\n",
      "Iteration 66, loss = 0.00501241\n",
      "Iteration 67, loss = 0.00768906\n",
      "Iteration 68, loss = 0.00491361\n",
      "Iteration 69, loss = 0.00398925\n",
      "Iteration 70, loss = 0.00372875\n",
      "Iteration 71, loss = 0.00362483\n",
      "Iteration 72, loss = 0.00357771\n",
      "Iteration 73, loss = 0.00354477\n",
      "Iteration 74, loss = 0.00350462\n",
      "Iteration 75, loss = 0.00347092\n",
      "Iteration 76, loss = 0.00881986\n",
      "Iteration 77, loss = 0.00695263\n",
      "Iteration 78, loss = 0.00434459\n",
      "Iteration 79, loss = 0.00361522\n",
      "Iteration 80, loss = 0.00351092\n",
      "Iteration 81, loss = 0.00344637\n",
      "Iteration 82, loss = 0.00340711\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8555176336746303\n",
      "Iteration 1, loss = 0.09168883\n",
      "Iteration 2, loss = 0.06947616\n",
      "Iteration 3, loss = 0.06792934\n",
      "Iteration 4, loss = 0.06645211\n",
      "Iteration 5, loss = 0.06413658\n",
      "Iteration 6, loss = 0.05942618\n",
      "Iteration 7, loss = 0.05322025\n",
      "Iteration 8, loss = 0.04694754\n",
      "Iteration 9, loss = 0.04358849\n",
      "Iteration 10, loss = 0.04103067\n",
      "Iteration 11, loss = 0.03960420\n",
      "Iteration 12, loss = 0.03789265\n",
      "Iteration 13, loss = 0.03675824\n",
      "Iteration 14, loss = 0.03567219\n",
      "Iteration 15, loss = 0.03371090\n",
      "Iteration 16, loss = 0.03279806\n",
      "Iteration 17, loss = 0.03047415\n",
      "Iteration 18, loss = 0.02931798\n",
      "Iteration 19, loss = 0.02861170\n",
      "Iteration 20, loss = 0.02718578\n",
      "Iteration 21, loss = 0.02595793\n",
      "Iteration 22, loss = 0.02494606\n",
      "Iteration 23, loss = 0.02339085\n",
      "Iteration 24, loss = 0.02232792\n",
      "Iteration 25, loss = 0.02164680\n",
      "Iteration 26, loss = 0.02077122\n",
      "Iteration 27, loss = 0.02004780\n",
      "Iteration 28, loss = 0.01835760\n",
      "Iteration 29, loss = 0.01792633\n",
      "Iteration 30, loss = 0.01695806\n",
      "Iteration 31, loss = 0.01639088\n",
      "Iteration 32, loss = 0.01528650\n",
      "Iteration 33, loss = 0.01448634\n",
      "Iteration 34, loss = 0.01395089\n",
      "Iteration 35, loss = 0.01357065\n",
      "Iteration 36, loss = 0.01224020\n",
      "Iteration 37, loss = 0.01202028\n",
      "Iteration 38, loss = 0.01104866\n",
      "Iteration 39, loss = 0.01031439\n",
      "Iteration 40, loss = 0.01002858\n",
      "Iteration 41, loss = 0.00932841\n",
      "Iteration 42, loss = 0.00911155\n",
      "Iteration 43, loss = 0.00854894\n",
      "Iteration 44, loss = 0.00776765\n",
      "Iteration 45, loss = 0.00777639\n",
      "Iteration 46, loss = 0.00703760\n",
      "Iteration 47, loss = 0.00695069\n",
      "Iteration 48, loss = 0.00634784\n",
      "Iteration 49, loss = 0.00625870\n",
      "Iteration 50, loss = 0.00608780\n",
      "Iteration 51, loss = 0.00564555\n",
      "Iteration 52, loss = 0.00548262\n",
      "Iteration 53, loss = 0.00524696\n",
      "Iteration 54, loss = 0.00513311\n",
      "Iteration 55, loss = 0.00488879\n",
      "Iteration 56, loss = 0.00473133\n",
      "Iteration 57, loss = 0.00539618\n",
      "Iteration 58, loss = 0.00507792\n",
      "Iteration 59, loss = 0.00462999\n",
      "Iteration 60, loss = 0.00424784\n",
      "Iteration 61, loss = 0.00411661\n",
      "Iteration 62, loss = 0.00392638\n",
      "Iteration 63, loss = 0.00450656\n",
      "Iteration 64, loss = 0.00730281\n",
      "Iteration 65, loss = 0.00574074\n",
      "Iteration 66, loss = 0.00401175\n",
      "Iteration 67, loss = 0.00382742\n",
      "Iteration 68, loss = 0.00363183\n",
      "Iteration 69, loss = 0.00354920\n",
      "Iteration 70, loss = 0.00351162\n",
      "Iteration 71, loss = 0.00349569\n",
      "Iteration 72, loss = 0.00343104\n",
      "Iteration 73, loss = 0.00370694\n",
      "Iteration 74, loss = 0.00775571\n",
      "Iteration 75, loss = 0.00552932\n",
      "Iteration 76, loss = 0.00415063\n",
      "Iteration 77, loss = 0.00361452\n",
      "Iteration 78, loss = 0.00341869\n",
      "Iteration 79, loss = 0.00334434\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8622502628811776\n",
      "Mean F1 score: 0.8667166190248095\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "f1_scores = []\n",
    "for train_indices, test_indices in kf.split(x_train):\n",
    "    clf.fit(x_train_oh.iloc[train_indices, 1:], x_train_oh.iloc[train_indices, 0])\n",
    "    pred = clf.predict(x_train_oh.iloc[test_indices, 1:])\n",
    "    #print(clf.score(x_train_oh.iloc[test_indices, 1:], x_train_oh.iloc[test_indices, 0]))\n",
    "    f1_scores.append(f1_score(pred, x_train_oh.iloc[test_indices, 0]))\n",
    "    print(f1_score(pred, x_train_oh.iloc[test_indices, 0]))\n",
    "print(\"Mean F1 score:\", np.array(f1_scores).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot convergence of solver\n",
    "\n",
    "#(0.005, 0.005) -> 0.86497\n",
    "#(0.005, 0.006) -> 0.87049\n",
    "#(0.005, 0.007) -> 0.86912\n",
    "#(6, 6) -> 0.86409\n",
    "#(4,6) -> 0.86950\n",
    "#(4,5) -> 0.87112\n",
    "\n",
    "######\n",
    "\n",
    "#(3,5) -> 0.8667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.07503502\n",
      "Iteration 2, loss = 0.06145012\n",
      "Iteration 3, loss = 0.04710899\n",
      "Iteration 4, loss = 0.04028022\n",
      "Iteration 5, loss = 0.03630861\n",
      "Iteration 6, loss = 0.03274113\n",
      "Iteration 7, loss = 0.03041653\n",
      "Iteration 8, loss = 0.02792677\n",
      "Iteration 9, loss = 0.02584370\n",
      "Iteration 10, loss = 0.02379323\n",
      "Iteration 11, loss = 0.02268267\n",
      "Iteration 12, loss = 0.02123426\n",
      "Iteration 13, loss = 0.02009154\n",
      "Iteration 14, loss = 0.01913667\n",
      "Iteration 15, loss = 0.01825758\n",
      "Iteration 16, loss = 0.01731142\n",
      "Iteration 17, loss = 0.01668871\n",
      "Iteration 18, loss = 0.01576886\n",
      "Iteration 19, loss = 0.01502045\n",
      "Iteration 20, loss = 0.01433648\n",
      "Iteration 21, loss = 0.01392372\n",
      "Iteration 22, loss = 0.01318431\n",
      "Iteration 23, loss = 0.01243758\n",
      "Iteration 24, loss = 0.01186825\n",
      "Iteration 25, loss = 0.01114729\n",
      "Iteration 26, loss = 0.01086676\n",
      "Iteration 27, loss = 0.01071242\n",
      "Iteration 28, loss = 0.00998329\n",
      "Iteration 29, loss = 0.00950901\n",
      "Iteration 30, loss = 0.00914218\n",
      "Iteration 31, loss = 0.00885639\n",
      "Iteration 32, loss = 0.00868206\n",
      "Iteration 33, loss = 0.00806015\n",
      "Iteration 34, loss = 0.00806630\n",
      "Iteration 35, loss = 0.00795834\n",
      "Iteration 36, loss = 0.00741820\n",
      "Iteration 37, loss = 0.00727779\n",
      "Iteration 38, loss = 0.00762884\n",
      "Iteration 39, loss = 0.00779277\n",
      "Iteration 40, loss = 0.00666403\n",
      "Iteration 41, loss = 0.00655851\n",
      "Iteration 42, loss = 0.00727173\n",
      "Iteration 43, loss = 0.00750473\n",
      "Iteration 44, loss = 0.00615704\n",
      "Iteration 45, loss = 0.00622461\n",
      "Iteration 46, loss = 0.00709222\n",
      "Iteration 47, loss = 0.00762281\n",
      "Iteration 48, loss = 0.00587392\n",
      "Iteration 49, loss = 0.00521486\n",
      "Iteration 50, loss = 0.00527388\n",
      "Iteration 51, loss = 0.01177064\n",
      "Iteration 52, loss = 0.00637542\n",
      "Iteration 53, loss = 0.00524358\n",
      "Iteration 54, loss = 0.00521998\n",
      "Iteration 55, loss = 0.00552387\n",
      "Iteration 56, loss = 0.00964569\n",
      "Iteration 57, loss = 0.00650608\n",
      "Iteration 58, loss = 0.00557428\n",
      "Iteration 59, loss = 0.00493716\n",
      "Iteration 60, loss = 0.00495827\n",
      "Iteration 61, loss = 0.00978754\n",
      "Iteration 62, loss = 0.00635574\n",
      "Iteration 63, loss = 0.00532951\n",
      "Iteration 64, loss = 0.00487887\n",
      "Iteration 65, loss = 0.00467104\n",
      "Iteration 66, loss = 0.00876611\n",
      "Iteration 67, loss = 0.00799300\n",
      "Iteration 68, loss = 0.00547713\n",
      "Iteration 69, loss = 0.00475532\n",
      "Iteration 70, loss = 0.00449896\n",
      "Iteration 71, loss = 0.00826031\n",
      "Iteration 72, loss = 0.00863204\n",
      "Iteration 73, loss = 0.00523143\n",
      "Iteration 74, loss = 0.00465915\n",
      "Iteration 75, loss = 0.00443713\n",
      "Iteration 76, loss = 0.00481131\n",
      "Iteration 77, loss = 0.01227051\n",
      "Iteration 78, loss = 0.00576016\n",
      "Iteration 79, loss = 0.00490587\n",
      "Iteration 80, loss = 0.00456509\n",
      "Iteration 81, loss = 0.00437798\n",
      "Iteration 82, loss = 0.00910923\n",
      "Iteration 83, loss = 0.00732666\n",
      "Iteration 84, loss = 0.00507049\n",
      "Iteration 85, loss = 0.00447669\n",
      "Iteration 86, loss = 0.00425044\n",
      "Iteration 87, loss = 0.00413352\n",
      "Iteration 88, loss = 0.01184762\n",
      "Iteration 89, loss = 0.00689303\n",
      "Iteration 90, loss = 0.00484750\n",
      "Iteration 91, loss = 0.00435131\n",
      "Iteration 92, loss = 0.00418373\n",
      "Iteration 93, loss = 0.00409113\n",
      "Iteration 94, loss = 0.01204772\n",
      "Iteration 95, loss = 0.00611099\n",
      "Iteration 96, loss = 0.00476622\n",
      "Iteration 97, loss = 0.00427679\n",
      "Iteration 98, loss = 0.00410499\n",
      "Iteration 99, loss = 0.00401470\n",
      "Iteration 100, loss = 0.01266489\n",
      "Iteration 101, loss = 0.00688595\n",
      "Iteration 102, loss = 0.00483297\n",
      "Iteration 103, loss = 0.00437753\n",
      "Iteration 104, loss = 0.00411234\n",
      "Iteration 105, loss = 0.00397729\n",
      "Iteration 106, loss = 0.00435380\n",
      "Iteration 107, loss = 0.01382245\n",
      "Iteration 108, loss = 0.00582258\n",
      "Iteration 109, loss = 0.00446888\n",
      "Iteration 110, loss = 0.00415901\n",
      "Iteration 111, loss = 0.00398992\n",
      "Iteration 112, loss = 0.00406548\n",
      "Iteration 113, loss = 0.01203140\n",
      "Iteration 114, loss = 0.00608061\n",
      "Iteration 115, loss = 0.00463256\n",
      "Iteration 116, loss = 0.00411688\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "clf.fit(x_train_oh.iloc[:, 1:], x_train_oh['Active'])\n",
    "pred = clf.predict(x_test_oh)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "...   ..\n",
      "47995  0\n",
      "47996  0\n",
      "47997  0\n",
      "47998  0\n",
      "47999  0\n",
      "\n",
      "[48000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(pred) \n",
    "print(df)\n",
    "df.to_csv('out.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search to find parameters that give best F1 score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  15 | elapsed: 11.4min remaining:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed: 11.8min remaining:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed: 13.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.09131621\n",
      "Iteration 2, loss = 0.06761211\n",
      "Iteration 3, loss = 0.06455770\n",
      "Iteration 4, loss = 0.05867045\n",
      "Iteration 5, loss = 0.05130971\n",
      "Iteration 6, loss = 0.04483593\n",
      "Iteration 7, loss = 0.04162982\n",
      "Iteration 8, loss = 0.03968641\n",
      "Iteration 9, loss = 0.03764739\n",
      "Iteration 10, loss = 0.03619775\n",
      "Iteration 11, loss = 0.03455598\n",
      "Iteration 12, loss = 0.03267596\n",
      "Iteration 13, loss = 0.03144329\n",
      "Iteration 14, loss = 0.02974056\n",
      "Iteration 15, loss = 0.02835114\n",
      "Iteration 16, loss = 0.02697558\n",
      "Iteration 17, loss = 0.02543291\n",
      "Iteration 18, loss = 0.02421594\n",
      "Iteration 19, loss = 0.02296975\n",
      "Iteration 20, loss = 0.02196010\n",
      "Iteration 21, loss = 0.02080479\n",
      "Iteration 22, loss = 0.01969732\n",
      "Iteration 23, loss = 0.01875770\n",
      "Iteration 24, loss = 0.01833318\n",
      "Iteration 25, loss = 0.01701710\n",
      "Iteration 26, loss = 0.01614367\n",
      "Iteration 27, loss = 0.01543255\n",
      "Iteration 28, loss = 0.01481114\n",
      "Iteration 29, loss = 0.01403652\n",
      "Iteration 30, loss = 0.01344021\n",
      "Iteration 31, loss = 0.01260375\n",
      "Iteration 32, loss = 0.01215999\n",
      "Iteration 33, loss = 0.01149848\n",
      "Iteration 34, loss = 0.01104077\n",
      "Iteration 35, loss = 0.01014170\n",
      "Iteration 36, loss = 0.00988263\n",
      "Iteration 37, loss = 0.00923935\n",
      "Iteration 38, loss = 0.00879502\n",
      "Iteration 39, loss = 0.00832982\n",
      "Iteration 40, loss = 0.00803419\n",
      "Iteration 41, loss = 0.00751666\n",
      "Iteration 42, loss = 0.00720137\n",
      "Iteration 43, loss = 0.00681315\n",
      "Iteration 44, loss = 0.00650460\n",
      "Iteration 45, loss = 0.00621687\n",
      "Iteration 46, loss = 0.00578584\n",
      "Iteration 47, loss = 0.00574972\n",
      "Iteration 48, loss = 0.00544583\n",
      "Iteration 49, loss = 0.00540436\n",
      "Iteration 50, loss = 0.00522263\n",
      "Iteration 51, loss = 0.00522014\n",
      "Iteration 52, loss = 0.00492444\n",
      "Iteration 53, loss = 0.00469869\n",
      "Iteration 54, loss = 0.00452087\n",
      "Iteration 55, loss = 0.00435654\n",
      "Iteration 56, loss = 0.00452020\n",
      "Iteration 57, loss = 0.00515324\n",
      "Iteration 58, loss = 0.00505374\n",
      "Iteration 59, loss = 0.00422076\n",
      "Iteration 60, loss = 0.00396159\n",
      "Iteration 61, loss = 0.00379624\n",
      "Iteration 62, loss = 0.00372380\n",
      "Iteration 63, loss = 0.00396110\n",
      "Iteration 64, loss = 0.00609549\n",
      "Iteration 65, loss = 0.00600485\n",
      "Iteration 66, loss = 0.00386858\n",
      "Iteration 67, loss = 0.00357650\n",
      "Iteration 68, loss = 0.00350682\n",
      "Iteration 69, loss = 0.00346354\n",
      "Iteration 70, loss = 0.00341637\n",
      "Iteration 71, loss = 0.00341785\n",
      "Iteration 72, loss = 0.00335343\n",
      "Iteration 73, loss = 0.00430725\n",
      "Iteration 74, loss = 0.00717303\n",
      "Iteration 75, loss = 0.00409006\n",
      "Iteration 76, loss = 0.00353953\n",
      "Iteration 77, loss = 0.00334195\n",
      "Iteration 78, loss = 0.00327236\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Parameters {'hidden_layer_sizes': (180, 120)} gives best score: 0.8762398469570061\n",
      "{'mean_fit_time': array([227.36460304, 243.43706369, 164.37502666]), 'std_fit_time': array([ 8.09345012, 14.34418735, 25.72735622]), 'mean_score_time': array([0.2640902 , 0.29350772, 0.18165436]), 'std_score_time': array([0.01798471, 0.0147035 , 0.05500446]), 'param_hidden_layer_sizes': masked_array(data=[(180, 120), (240, 120), (180, 90)],\n",
      "             mask=[False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'hidden_layer_sizes': (180, 120)}, {'hidden_layer_sizes': (240, 120)}, {'hidden_layer_sizes': (180, 90)}], 'split0_test_score': array([0.8876652 , 0.87909605, 0.87162162]), 'split1_test_score': array([0.8980963 , 0.88444444, 0.87317621]), 'split2_test_score': array([0.8745838 , 0.86510591, 0.85264342]), 'split3_test_score': array([0.85779295, 0.86492622, 0.87671233]), 'split4_test_score': array([0.86306099, 0.84650113, 0.861678  ]), 'mean_test_score': array([0.87623985, 0.86801475, 0.86716632]), 'std_test_score': array([0.01499971, 0.01321791, 0.00881057]), 'rank_test_score': array([1, 2, 3], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "l1_size = int(0.005 * x_train_oh.shape[0])\n",
    "l2_size = int(0.005 * x_train_oh.shape[0])\n",
    "\n",
    "mlp = MLPClassifier(random_state=1, alpha=8e-4, max_iter=200, solver='adam', activation='tanh', verbose=10)\n",
    "\n",
    "# All parameters we want to try:\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(int(0.003 * x_train_oh.shape[0]), int(0.002 * x_train_oh.shape[0])),\n",
    "                           (int(0.004 * x_train_oh.shape[0]), int(0.002 * x_train_oh.shape[0])),\n",
    "                           (int(0.003 * x_train_oh.shape[0]), int(0.0015 * x_train_oh.shape[0]))],\n",
    "    #'activation': ['tanh', 'relu', 'logistic'],\n",
    "    #'solver': ['sgd', 'adam'],\n",
    "    #'alpha': [1e-3, 6e-4, 8e-4],\n",
    "}\n",
    "\n",
    "# Do grid search over all parameter options:\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=5, scoring='f1', verbose=10)\n",
    "clf.fit(x_train_oh.iloc[:, 1:], x_train_oh['Active'])\n",
    "print('Parameters', clf.best_params_, \"gives best score:\", clf.best_score_)\n",
    "print(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \"the optimal size of the hidden layer is usually between the size of the input and size of the output layers\"\n",
    "#  \"number of neurons = 0.005 * number of samples?\" 2/3 size of input is also usual\n",
    "\n",
    "# Easy: 0.607427055703\n",
    "# Medium: 0.852643419573\n",
    "# Hard: 0.89591280654"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
