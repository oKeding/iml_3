{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and get data as letters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Active  0  1  2  3\n",
      "0           0  D  K  W  L\n",
      "1           0  F  C  H  N\n",
      "2           0  K  D  Q  P\n",
      "3           0  F  N  W  I\n",
      "4           0  N  K  R  M\n",
      "...       ... .. .. .. ..\n",
      "59995       0  A  H  S  P\n",
      "59996       0  T  G  E  L\n",
      "59997       0  H  C  N  C\n",
      "59998       0  G  D  E  E\n",
      "59999       0  F  I  N  M\n",
      "\n",
      "[60000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "test = False\n",
    "\n",
    "x_train = pd.read_csv('train.csv')\n",
    "\n",
    "x_train = x_train[0:60000]\n",
    "\n",
    "letters = x_train['Sequence'].apply(lambda x: pd.Series(list(x)))\n",
    "x_train = x_train.drop('Sequence', axis = 1)\n",
    "x_train = x_train.join(letters)\n",
    "print(x_train)\n",
    "\n",
    "if test:\n",
    "    x_test = pd.read_csv('test.csv')\n",
    "    letters = x_test['Sequence'].apply(lambda x: pd.Series(list(x)))\n",
    "    x_test = x_test.drop('Sequence', axis = 1)\n",
    "    x_test = x_test.join(letters)\n",
    "    print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Active  0_A  0_C  0_D  0_E  0_F  0_G  0_H  0_I  0_K  ...  3_M  3_N  \\\n",
      "0           0    0    0    1    0    0    0    0    0    0  ...    0    0   \n",
      "1           0    0    0    0    0    1    0    0    0    0  ...    0    1   \n",
      "2           0    0    0    0    0    0    0    0    0    1  ...    0    0   \n",
      "3           0    0    0    0    0    1    0    0    0    0  ...    0    0   \n",
      "4           0    0    0    0    0    0    0    0    0    0  ...    1    0   \n",
      "...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "59995       0    1    0    0    0    0    0    0    0    0  ...    0    0   \n",
      "59996       0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
      "59997       0    0    0    0    0    0    0    1    0    0  ...    0    0   \n",
      "59998       0    0    0    0    0    0    1    0    0    0  ...    0    0   \n",
      "59999       0    0    0    0    0    1    0    0    0    0  ...    1    0   \n",
      "\n",
      "       3_P  3_Q  3_R  3_S  3_T  3_V  3_W  3_Y  \n",
      "0        0    0    0    0    0    0    0    0  \n",
      "1        0    0    0    0    0    0    0    0  \n",
      "2        1    0    0    0    0    0    0    0  \n",
      "3        0    0    0    0    0    0    0    0  \n",
      "4        0    0    0    0    0    0    0    0  \n",
      "...    ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "59995    1    0    0    0    0    0    0    0  \n",
      "59996    0    0    0    0    0    0    0    0  \n",
      "59997    0    0    0    0    0    0    0    0  \n",
      "59998    0    0    0    0    0    0    0    0  \n",
      "59999    0    0    0    0    0    0    0    0  \n",
      "\n",
      "[60000 rows x 81 columns]\n"
     ]
    }
   ],
   "source": [
    "x_train_oh = pd.get_dummies(data=x_train, columns=[0, 1, 2, 3], sparse=True)\n",
    "print(x_train_oh)\n",
    "\n",
    "if test:\n",
    "    x_test_oh = pd.get_dummies(data=x_test, columns=[0, 1, 2, 3], sparse=True)\n",
    "    print(x_test_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix unbalanced data for better performance. Easiest is to oversample the 1's. Its worth a shot at least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Neural network <-- there is documentation in scikit\n",
    "l1_size = int(0.003 * x_train_oh.shape[0])\n",
    "l2_size = int(0.005 * x_train_oh.shape[0])\n",
    "clf = MLPClassifier(solver='adam', alpha=8e-4, verbose=10, hidden_layer_sizes=(l1_size,l2_size), random_state=1, activation='tanh', max_iter=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV F1 score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.09410011\n",
      "Iteration 2, loss = 0.06921685\n",
      "Iteration 3, loss = 0.06852413\n",
      "Iteration 4, loss = 0.06713904\n",
      "Iteration 5, loss = 0.06482222\n",
      "Iteration 6, loss = 0.05972665\n",
      "Iteration 7, loss = 0.05309142\n",
      "Iteration 8, loss = 0.04733377\n",
      "Iteration 9, loss = 0.04381315\n",
      "Iteration 10, loss = 0.04171600\n",
      "Iteration 11, loss = 0.04034404\n",
      "Iteration 12, loss = 0.03908368\n",
      "Iteration 13, loss = 0.03804557\n",
      "Iteration 14, loss = 0.03647994\n",
      "Iteration 15, loss = 0.03511197\n",
      "Iteration 16, loss = 0.03407346\n",
      "Iteration 17, loss = 0.03227091\n",
      "Iteration 18, loss = 0.03136344\n",
      "Iteration 19, loss = 0.02979605\n",
      "Iteration 20, loss = 0.02928689\n",
      "Iteration 21, loss = 0.02808438\n",
      "Iteration 22, loss = 0.02701508\n",
      "Iteration 23, loss = 0.02606442\n",
      "Iteration 24, loss = 0.02488144\n",
      "Iteration 25, loss = 0.02378730\n",
      "Iteration 26, loss = 0.02311449\n",
      "Iteration 27, loss = 0.02252202\n",
      "Iteration 28, loss = 0.02146177\n",
      "Iteration 29, loss = 0.02036859\n",
      "Iteration 30, loss = 0.01992482\n",
      "Iteration 31, loss = 0.01902458\n",
      "Iteration 32, loss = 0.01802458\n",
      "Iteration 33, loss = 0.01731077\n",
      "Iteration 34, loss = 0.01646403\n",
      "Iteration 35, loss = 0.01560323\n",
      "Iteration 36, loss = 0.01493568\n",
      "Iteration 37, loss = 0.01443928\n",
      "Iteration 38, loss = 0.01324090\n",
      "Iteration 39, loss = 0.01280257\n",
      "Iteration 40, loss = 0.01211569\n",
      "Iteration 41, loss = 0.01130405\n",
      "Iteration 42, loss = 0.01071294\n",
      "Iteration 43, loss = 0.01030470\n",
      "Iteration 44, loss = 0.00940780\n",
      "Iteration 45, loss = 0.00953349\n",
      "Iteration 46, loss = 0.00845929\n",
      "Iteration 47, loss = 0.00814091\n",
      "Iteration 48, loss = 0.00749766\n",
      "Iteration 49, loss = 0.00705706\n",
      "Iteration 50, loss = 0.00680121\n",
      "Iteration 51, loss = 0.00652355\n",
      "Iteration 52, loss = 0.00640505\n",
      "Iteration 53, loss = 0.00608017\n",
      "Iteration 54, loss = 0.00557404\n",
      "Iteration 55, loss = 0.00555867\n",
      "Iteration 56, loss = 0.00541419\n",
      "Iteration 57, loss = 0.00528246\n",
      "Iteration 58, loss = 0.00498460\n",
      "Iteration 59, loss = 0.00487268\n",
      "Iteration 60, loss = 0.00463495\n",
      "Iteration 61, loss = 0.00464166\n",
      "Iteration 62, loss = 0.00482221\n",
      "Iteration 63, loss = 0.00436320\n",
      "Iteration 64, loss = 0.00426966\n",
      "Iteration 65, loss = 0.00427924\n",
      "Iteration 66, loss = 0.00443575\n",
      "Iteration 67, loss = 0.00564315\n",
      "Iteration 68, loss = 0.00701276\n",
      "Iteration 69, loss = 0.00436509\n",
      "Iteration 70, loss = 0.00380573\n",
      "Iteration 71, loss = 0.00369593\n",
      "Iteration 72, loss = 0.00361815\n",
      "Iteration 73, loss = 0.00356139\n",
      "Iteration 74, loss = 0.00352892\n",
      "Iteration 75, loss = 0.00348151\n",
      "Iteration 76, loss = 0.00345024\n",
      "Iteration 77, loss = 0.00354394\n",
      "Iteration 78, loss = 0.00837325\n",
      "Iteration 79, loss = 0.00570409\n",
      "Iteration 80, loss = 0.00398565\n",
      "Iteration 81, loss = 0.00355593\n",
      "Iteration 82, loss = 0.00342528\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8692660550458715\n",
      "Iteration 1, loss = 0.09417419\n",
      "Iteration 2, loss = 0.07063553\n",
      "Iteration 3, loss = 0.06995332\n",
      "Iteration 4, loss = 0.06864437\n",
      "Iteration 5, loss = 0.06606171\n",
      "Iteration 6, loss = 0.06064346\n",
      "Iteration 7, loss = 0.05389515\n",
      "Iteration 8, loss = 0.04814612\n",
      "Iteration 9, loss = 0.04512867\n",
      "Iteration 10, loss = 0.04270001\n",
      "Iteration 11, loss = 0.04057262\n",
      "Iteration 12, loss = 0.03920433\n",
      "Iteration 13, loss = 0.03757135\n",
      "Iteration 14, loss = 0.03547102\n",
      "Iteration 15, loss = 0.03456757\n",
      "Iteration 16, loss = 0.03348519\n",
      "Iteration 17, loss = 0.03159633\n",
      "Iteration 18, loss = 0.03104330\n",
      "Iteration 19, loss = 0.02944419\n",
      "Iteration 20, loss = 0.02836466\n",
      "Iteration 21, loss = 0.02755468\n",
      "Iteration 22, loss = 0.02661571\n",
      "Iteration 23, loss = 0.02536067\n",
      "Iteration 24, loss = 0.02439429\n",
      "Iteration 25, loss = 0.02287346\n",
      "Iteration 26, loss = 0.02283153\n",
      "Iteration 27, loss = 0.02200626\n",
      "Iteration 28, loss = 0.02093222\n",
      "Iteration 29, loss = 0.01992837\n",
      "Iteration 30, loss = 0.01930164\n",
      "Iteration 31, loss = 0.01834725\n",
      "Iteration 32, loss = 0.01716401\n",
      "Iteration 33, loss = 0.01664965\n",
      "Iteration 34, loss = 0.01606947\n",
      "Iteration 35, loss = 0.01575477\n",
      "Iteration 36, loss = 0.01495543\n",
      "Iteration 37, loss = 0.01387052\n",
      "Iteration 38, loss = 0.01309912\n",
      "Iteration 39, loss = 0.01255101\n",
      "Iteration 40, loss = 0.01224501\n",
      "Iteration 41, loss = 0.01122575\n",
      "Iteration 42, loss = 0.01048955\n",
      "Iteration 43, loss = 0.01038717\n",
      "Iteration 44, loss = 0.00952593\n",
      "Iteration 45, loss = 0.00929442\n",
      "Iteration 46, loss = 0.00874664\n",
      "Iteration 47, loss = 0.00873352\n",
      "Iteration 48, loss = 0.00761574\n",
      "Iteration 49, loss = 0.00746175\n",
      "Iteration 50, loss = 0.00712020\n",
      "Iteration 51, loss = 0.00663160\n",
      "Iteration 52, loss = 0.00684383\n",
      "Iteration 53, loss = 0.00620930\n",
      "Iteration 54, loss = 0.00611257\n",
      "Iteration 55, loss = 0.00588183\n",
      "Iteration 56, loss = 0.00570654\n",
      "Iteration 57, loss = 0.00521377\n",
      "Iteration 58, loss = 0.00531189\n",
      "Iteration 59, loss = 0.00520888\n",
      "Iteration 60, loss = 0.00491581\n",
      "Iteration 61, loss = 0.00575668\n",
      "Iteration 62, loss = 0.00523427\n",
      "Iteration 63, loss = 0.00491519\n",
      "Iteration 64, loss = 0.00447796\n",
      "Iteration 65, loss = 0.00418207\n",
      "Iteration 66, loss = 0.00408492\n",
      "Iteration 67, loss = 0.00408053\n",
      "Iteration 68, loss = 0.00412324\n",
      "Iteration 69, loss = 0.00879493\n",
      "Iteration 70, loss = 0.00698843\n",
      "Iteration 71, loss = 0.00430495\n",
      "Iteration 72, loss = 0.00391782\n",
      "Iteration 73, loss = 0.00381239\n",
      "Iteration 74, loss = 0.00370788\n",
      "Iteration 75, loss = 0.00364871\n",
      "Iteration 76, loss = 0.00362514\n",
      "Iteration 77, loss = 0.00368213\n",
      "Iteration 78, loss = 0.00395455\n",
      "Iteration 79, loss = 0.00424211\n",
      "Iteration 80, loss = 0.00982344\n",
      "Iteration 81, loss = 0.00499761\n",
      "Iteration 82, loss = 0.00380340\n",
      "Iteration 83, loss = 0.00360573\n",
      "Iteration 84, loss = 0.00352747\n",
      "Iteration 85, loss = 0.00346703\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.889406286379511\n",
      "Iteration 1, loss = 0.09232326\n",
      "Iteration 2, loss = 0.06906640\n",
      "Iteration 3, loss = 0.06787852\n",
      "Iteration 4, loss = 0.06662755\n",
      "Iteration 5, loss = 0.06405439\n",
      "Iteration 6, loss = 0.05948407\n",
      "Iteration 7, loss = 0.05291996\n",
      "Iteration 8, loss = 0.04682791\n",
      "Iteration 9, loss = 0.04406677\n",
      "Iteration 10, loss = 0.04244998\n",
      "Iteration 11, loss = 0.04038548\n",
      "Iteration 12, loss = 0.03867391\n",
      "Iteration 13, loss = 0.03665116\n",
      "Iteration 14, loss = 0.03487382\n",
      "Iteration 15, loss = 0.03325325\n",
      "Iteration 16, loss = 0.03152592\n",
      "Iteration 17, loss = 0.03011441\n",
      "Iteration 18, loss = 0.02889898\n",
      "Iteration 19, loss = 0.02788355\n",
      "Iteration 20, loss = 0.02665466\n",
      "Iteration 21, loss = 0.02582031\n",
      "Iteration 22, loss = 0.02486127\n",
      "Iteration 23, loss = 0.02400826\n",
      "Iteration 24, loss = 0.02344135\n",
      "Iteration 25, loss = 0.02145984\n",
      "Iteration 26, loss = 0.02099633\n",
      "Iteration 27, loss = 0.02036319\n",
      "Iteration 28, loss = 0.01943741\n",
      "Iteration 29, loss = 0.01802049\n",
      "Iteration 30, loss = 0.01759127\n",
      "Iteration 31, loss = 0.01668837\n",
      "Iteration 32, loss = 0.01563277\n",
      "Iteration 33, loss = 0.01473536\n",
      "Iteration 34, loss = 0.01472666\n",
      "Iteration 35, loss = 0.01410070\n",
      "Iteration 36, loss = 0.01340029\n",
      "Iteration 37, loss = 0.01219149\n",
      "Iteration 38, loss = 0.01125900\n",
      "Iteration 39, loss = 0.01084730\n",
      "Iteration 40, loss = 0.01066349\n",
      "Iteration 41, loss = 0.00991256\n",
      "Iteration 42, loss = 0.00895835\n",
      "Iteration 43, loss = 0.00885417\n",
      "Iteration 44, loss = 0.00821700\n",
      "Iteration 45, loss = 0.00794457\n",
      "Iteration 46, loss = 0.00742701\n",
      "Iteration 47, loss = 0.00720847\n",
      "Iteration 48, loss = 0.00675985\n",
      "Iteration 49, loss = 0.00630564\n",
      "Iteration 50, loss = 0.00619246\n",
      "Iteration 51, loss = 0.00564923\n",
      "Iteration 52, loss = 0.00542752\n",
      "Iteration 53, loss = 0.00562793\n",
      "Iteration 54, loss = 0.00540044\n",
      "Iteration 55, loss = 0.00511645\n",
      "Iteration 56, loss = 0.00515961\n",
      "Iteration 57, loss = 0.00483567\n",
      "Iteration 58, loss = 0.00478151\n",
      "Iteration 59, loss = 0.00456168\n",
      "Iteration 60, loss = 0.00541399\n",
      "Iteration 61, loss = 0.00534694\n",
      "Iteration 62, loss = 0.00433109\n",
      "Iteration 63, loss = 0.00392179\n",
      "Iteration 64, loss = 0.00380831\n",
      "Iteration 65, loss = 0.00375436\n",
      "Iteration 66, loss = 0.00377886\n",
      "Iteration 67, loss = 0.00421403\n",
      "Iteration 68, loss = 0.00849510\n",
      "Iteration 69, loss = 0.00623077\n",
      "Iteration 70, loss = 0.00403284\n",
      "Iteration 71, loss = 0.00370958\n",
      "Iteration 72, loss = 0.00358924\n",
      "Iteration 73, loss = 0.00354166\n",
      "Iteration 74, loss = 0.00346885\n",
      "Iteration 75, loss = 0.00342555\n",
      "Iteration 76, loss = 0.00339988\n",
      "Iteration 77, loss = 0.00338070\n",
      "Iteration 78, loss = 0.00341285\n",
      "Iteration 79, loss = 0.00896617\n",
      "Iteration 80, loss = 0.00739254\n",
      "Iteration 81, loss = 0.00405296\n",
      "Iteration 82, loss = 0.00353034\n",
      "Iteration 83, loss = 0.00341164\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8571428571428571\n",
      "Iteration 1, loss = 0.09112473\n",
      "Iteration 2, loss = 0.06896071\n",
      "Iteration 3, loss = 0.06750250\n",
      "Iteration 4, loss = 0.06620253\n",
      "Iteration 5, loss = 0.06340671\n",
      "Iteration 6, loss = 0.05890043\n",
      "Iteration 7, loss = 0.05204957\n",
      "Iteration 8, loss = 0.04639392\n",
      "Iteration 9, loss = 0.04305700\n",
      "Iteration 10, loss = 0.04088272\n",
      "Iteration 11, loss = 0.03922892\n",
      "Iteration 12, loss = 0.03774947\n",
      "Iteration 13, loss = 0.03694098\n",
      "Iteration 14, loss = 0.03536747\n",
      "Iteration 15, loss = 0.03437985\n",
      "Iteration 16, loss = 0.03313023\n",
      "Iteration 17, loss = 0.03159372\n",
      "Iteration 18, loss = 0.03085370\n",
      "Iteration 19, loss = 0.02926045\n",
      "Iteration 20, loss = 0.02804773\n",
      "Iteration 21, loss = 0.02721434\n",
      "Iteration 22, loss = 0.02662399\n",
      "Iteration 23, loss = 0.02473203\n",
      "Iteration 24, loss = 0.02412205\n",
      "Iteration 25, loss = 0.02271848\n",
      "Iteration 26, loss = 0.02204748\n",
      "Iteration 27, loss = 0.02119066\n",
      "Iteration 28, loss = 0.02002431\n",
      "Iteration 29, loss = 0.01921070\n",
      "Iteration 30, loss = 0.01857154\n",
      "Iteration 31, loss = 0.01763925\n",
      "Iteration 32, loss = 0.01657380\n",
      "Iteration 33, loss = 0.01606596\n",
      "Iteration 34, loss = 0.01540936\n",
      "Iteration 35, loss = 0.01522833\n",
      "Iteration 36, loss = 0.01404140\n",
      "Iteration 37, loss = 0.01357273\n",
      "Iteration 38, loss = 0.01244766\n",
      "Iteration 39, loss = 0.01194555\n",
      "Iteration 40, loss = 0.01162190\n",
      "Iteration 41, loss = 0.01069813\n",
      "Iteration 42, loss = 0.01018560\n",
      "Iteration 43, loss = 0.00981022\n",
      "Iteration 44, loss = 0.00903035\n",
      "Iteration 45, loss = 0.00889277\n",
      "Iteration 46, loss = 0.00818182\n",
      "Iteration 47, loss = 0.00828407\n",
      "Iteration 48, loss = 0.00754620\n",
      "Iteration 49, loss = 0.00720568\n",
      "Iteration 50, loss = 0.00672442\n",
      "Iteration 51, loss = 0.00634157\n",
      "Iteration 52, loss = 0.00599493\n",
      "Iteration 53, loss = 0.00612251\n",
      "Iteration 54, loss = 0.00604016\n",
      "Iteration 55, loss = 0.00561066\n",
      "Iteration 56, loss = 0.00579516\n",
      "Iteration 57, loss = 0.00522299\n",
      "Iteration 58, loss = 0.00483480\n",
      "Iteration 59, loss = 0.00470514\n",
      "Iteration 60, loss = 0.00492069\n",
      "Iteration 61, loss = 0.00458821\n",
      "Iteration 62, loss = 0.00443649\n",
      "Iteration 63, loss = 0.00422833\n",
      "Iteration 64, loss = 0.00418434\n",
      "Iteration 65, loss = 0.00402325\n",
      "Iteration 66, loss = 0.00501241\n",
      "Iteration 67, loss = 0.00768906\n",
      "Iteration 68, loss = 0.00491361\n",
      "Iteration 69, loss = 0.00398925\n",
      "Iteration 70, loss = 0.00372875\n",
      "Iteration 71, loss = 0.00362483\n",
      "Iteration 72, loss = 0.00357771\n",
      "Iteration 73, loss = 0.00354477\n",
      "Iteration 74, loss = 0.00350462\n",
      "Iteration 75, loss = 0.00347092\n",
      "Iteration 76, loss = 0.00881986\n",
      "Iteration 77, loss = 0.00695263\n",
      "Iteration 78, loss = 0.00434459\n",
      "Iteration 79, loss = 0.00361522\n",
      "Iteration 80, loss = 0.00351092\n",
      "Iteration 81, loss = 0.00344637\n",
      "Iteration 82, loss = 0.00340711\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8555176336746303\n",
      "Iteration 1, loss = 0.09168883\n",
      "Iteration 2, loss = 0.06947616\n",
      "Iteration 3, loss = 0.06792934\n",
      "Iteration 4, loss = 0.06645211\n",
      "Iteration 5, loss = 0.06413658\n",
      "Iteration 6, loss = 0.05942618\n",
      "Iteration 7, loss = 0.05322025\n",
      "Iteration 8, loss = 0.04694754\n",
      "Iteration 9, loss = 0.04358849\n",
      "Iteration 10, loss = 0.04103067\n",
      "Iteration 11, loss = 0.03960420\n",
      "Iteration 12, loss = 0.03789265\n",
      "Iteration 13, loss = 0.03675824\n",
      "Iteration 14, loss = 0.03567219\n",
      "Iteration 15, loss = 0.03371090\n",
      "Iteration 16, loss = 0.03279806\n",
      "Iteration 17, loss = 0.03047415\n",
      "Iteration 18, loss = 0.02931798\n",
      "Iteration 19, loss = 0.02861170\n",
      "Iteration 20, loss = 0.02718578\n",
      "Iteration 21, loss = 0.02595793\n",
      "Iteration 22, loss = 0.02494606\n",
      "Iteration 23, loss = 0.02339085\n",
      "Iteration 24, loss = 0.02232792\n",
      "Iteration 25, loss = 0.02164680\n",
      "Iteration 26, loss = 0.02077122\n",
      "Iteration 27, loss = 0.02004780\n",
      "Iteration 28, loss = 0.01835760\n",
      "Iteration 29, loss = 0.01792633\n",
      "Iteration 30, loss = 0.01695806\n",
      "Iteration 31, loss = 0.01639088\n",
      "Iteration 32, loss = 0.01528650\n",
      "Iteration 33, loss = 0.01448634\n",
      "Iteration 34, loss = 0.01395089\n",
      "Iteration 35, loss = 0.01357065\n",
      "Iteration 36, loss = 0.01224020\n",
      "Iteration 37, loss = 0.01202028\n",
      "Iteration 38, loss = 0.01104866\n",
      "Iteration 39, loss = 0.01031439\n",
      "Iteration 40, loss = 0.01002858\n",
      "Iteration 41, loss = 0.00932841\n",
      "Iteration 42, loss = 0.00911155\n",
      "Iteration 43, loss = 0.00854894\n",
      "Iteration 44, loss = 0.00776765\n",
      "Iteration 45, loss = 0.00777639\n",
      "Iteration 46, loss = 0.00703760\n",
      "Iteration 47, loss = 0.00695069\n",
      "Iteration 48, loss = 0.00634784\n",
      "Iteration 49, loss = 0.00625870\n",
      "Iteration 50, loss = 0.00608780\n",
      "Iteration 51, loss = 0.00564555\n",
      "Iteration 52, loss = 0.00548262\n",
      "Iteration 53, loss = 0.00524696\n",
      "Iteration 54, loss = 0.00513311\n",
      "Iteration 55, loss = 0.00488879\n",
      "Iteration 56, loss = 0.00473133\n",
      "Iteration 57, loss = 0.00539618\n",
      "Iteration 58, loss = 0.00507792\n",
      "Iteration 59, loss = 0.00462999\n",
      "Iteration 60, loss = 0.00424784\n",
      "Iteration 61, loss = 0.00411661\n",
      "Iteration 62, loss = 0.00392638\n",
      "Iteration 63, loss = 0.00450656\n",
      "Iteration 64, loss = 0.00730281\n",
      "Iteration 65, loss = 0.00574074\n",
      "Iteration 66, loss = 0.00401175\n",
      "Iteration 67, loss = 0.00382742\n",
      "Iteration 68, loss = 0.00363183\n",
      "Iteration 69, loss = 0.00354920\n",
      "Iteration 70, loss = 0.00351162\n",
      "Iteration 71, loss = 0.00349569\n",
      "Iteration 72, loss = 0.00343104\n",
      "Iteration 73, loss = 0.00370694\n",
      "Iteration 74, loss = 0.00775571\n",
      "Iteration 75, loss = 0.00552932\n",
      "Iteration 76, loss = 0.00415063\n",
      "Iteration 77, loss = 0.00361452\n",
      "Iteration 78, loss = 0.00341869\n",
      "Iteration 79, loss = 0.00334434\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8622502628811776\n",
      "Mean F1 score: 0.8667166190248095\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "f1_scores = []\n",
    "for train_indices, test_indices in kf.split(x_train):\n",
    "    clf.fit(x_train_oh.iloc[train_indices, 1:], x_train_oh.iloc[train_indices, 0])\n",
    "    pred = clf.predict(x_train_oh.iloc[test_indices, 1:])\n",
    "    #print(clf.score(x_train_oh.iloc[test_indices, 1:], x_train_oh.iloc[test_indices, 0]))\n",
    "    f1_scores.append(f1_score(pred, x_train_oh.iloc[test_indices, 0]))\n",
    "    print(f1_score(pred, x_train_oh.iloc[test_indices, 0]))\n",
    "print(\"Mean F1 score:\", np.array(f1_scores).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot convergence of solver\n",
    "\n",
    "#(0.005, 0.005) -> 0.86497\n",
    "#(0.005, 0.006) -> 0.87049\n",
    "#(0.005, 0.007) -> 0.86912\n",
    "#(6, 6) -> 0.86409\n",
    "#(4,6) -> 0.86950\n",
    "#(4,5) -> 0.87112\n",
    "\n",
    "######\n",
    "\n",
    "#(3,5) -> 0.8667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "clf.fit(x_train_oh.iloc[:, 1:], x_train_oh['Active'])\n",
    "pred = clf.predict(x_test_oh)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "...   ..\n",
      "47995  0\n",
      "47996  0\n",
      "47997  0\n",
      "47998  0\n",
      "47999  0\n",
      "\n",
      "[48000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(pred) \n",
    "print(df)\n",
    "df.to_csv('out.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0\n",
      "0  0\n",
      "1  0\n",
      "2  0\n",
      "3  0\n",
      "4  0\n",
      "5  1\n",
      "6  0\n",
      "7  0\n",
      "8  0\n",
      "9  0\n",
      "[0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(df[0:10])\n",
    "print(pred[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search to find parameters that give best F1 score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  9.4min\n"
     ]
    }
   ],
   "source": [
    "l1_size = int(0.005 * x_train_oh.shape[0])\n",
    "l2_size = int(0.005 * x_train_oh.shape[0])\n",
    "\n",
    "mlp = MLPClassifier(random_state=1, alpha=8e-4, max_iter=200, solver='adam', activation='tanh', verbose=10)\n",
    "\n",
    "# All parameters we want to try:\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(int(0.003 * x_train_oh.shape[0]), int(0.003 * x_train_oh.shape[0])),\n",
    "                           (int(0.002 * x_train_oh.shape[0]), int(0.003 * x_train_oh.shape[0])),\n",
    "                           (int(0.002 * x_train_oh.shape[0]), int(0.002 * x_train_oh.shape[0])),\n",
    "                           (int(0.003 * x_train_oh.shape[0]), int(0.002 * x_train_oh.shape[0]))],\n",
    "    #'activation': ['tanh', 'relu', 'logistic'],\n",
    "    #'solver': ['sgd', 'adam'],\n",
    "    #'alpha': [1e-3, 6e-4, 8e-4],\n",
    "}\n",
    "\n",
    "# Do grid search over all parameter options:\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=5, scoring='f1', verbose=10)\n",
    "clf.fit(x_train_oh.iloc[:, 1:], x_train_oh['Active'])\n",
    "print('Parameters', clf.best_params_, \"gives best score:\", clf.best_score_)\n",
    "print(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \"the optimal size of the hidden layer is usually between the size of the input and size of the output layers\"\n",
    "#  \"number of neurons = 0.005 * number of samples?\" 2/3 size of input is also usual\n",
    "\n",
    "# Easy: 0.607427055703\n",
    "# Medium: 0.852643419573\n",
    "# Hard: 0.89591280654"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_size = int(0.005 * x_train_oh.shape[0])\n",
    "l2_size = int(0.005 * x_train_oh.shape[0])\n",
    "\n",
    "mlp = MLPClassifier(random_state=1, alpha=8e-4, max_iter=200, solver='adam', activation='tanh', verbose=10)\n",
    "\n",
    "# All parameters we want to try:\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(int(0.004 * x_train_oh.shape[0]), int(0.006 * x_train_oh.shape[0])),\n",
    "                           (int(0.006 * x_train_oh.shape[0]), int(0.008 * x_train_oh.shape[0])),],\n",
    "    #'activation': ['tanh', 'relu', 'logistic'],\n",
    "    #'solver': ['sgd', 'adam'],\n",
    "    #'alpha': [1e-3, 6e-4, 8e-4],\n",
    "}\n",
    "\n",
    "# Do grid search over all parameter options:\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=5, scoring='f1', verbose=10)\n",
    "clf.fit(x_train_oh.iloc[:, 1:], x_train_oh['Active'])\n",
    "print('Parameters', clf.best_params_, \"gives best score:\", clf.best_score_)\n",
    "print(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
